# Measuring and Mitigating Social Biases in Language Models

## Contributors

Correspondence to: 
  - Paul Liang (pliang@cs.cmu.edu)
  - Chiyu Wu (chiyuwu23@gmail.com)

## Motivation

As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. This repository contains a set of tools to benchmark social biases in LMs.

## Datasets

## Evaluation metrics

### Fine-grained local biases

### High-level global biases
